<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Semantic Mapping for Navigation | Jianhao  Jiao</title>
    <meta name="author" content="Jianhao  Jiao">
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.3/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://gogojjh.github.io/projects/2024_semantic_mapping/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header --><header>

  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jianhao </span>Jiao</a>
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">About</a>
          </li>

          <!-- NOTE(gogojjh): -->
          

          <!-- Other pages -->
          <li class="nav-item ">
            <a class="nav-link" href="/projects/">Research Projects</a>
          </li>
          <li class="nav-item ">
            <a class="nav-link" href="/publications/">Publications</a>
          </li>
          <li class="nav-item ">
            <a class="nav-link" href="/resources/">Resources</a>
          </li>
          <li class="nav-item ">
            <a class="nav-link" href="/repositories/">Repositories</a>
          </li>
          <li class="nav-item ">
            <a class="nav-link" href="/cv/">CV</a>
          </li>

          <!-- Toogle theme mode -->
          <li class="toggle-container">
            <button id="light-toggle" title="Change theme">
              <i class="fas fa-moon"></i>
              <i class="fas fa-sun"></i>
            </button>
          </li>
        </ul>
      </div>
    </div>
  </nav>
  
  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>
</header>

    <!-- Content -->
    <div class="container mt-5">
      
        <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Semantic Mapping for Navigation</h1>
            <p class="post-description"></p>
          </header>

          <article>
            <h2 id="real-time-metric-semantic-mapping-for-autonomous-navigation-in-outdoor-environments">Real-Time Metric-Semantic Mapping for Autonomous Navigation in Outdoor Environments</h2>

<p><strong>Author</strong>: Jianhao Jiao, Ruoyu Geng, Yuanhang Li, Ren Xin, Bowen Yang, Jin Wu,
Dimitrios Kanoulas, Rui Fan, etc.</p>

<p><strong>Acknowledgement</strong>: This is a joint research work under the support from HKUST, HKUST(GZ), Tongji University, and University College London.</p>

<h4>
Please click the text for 
the 
open-source 
<a href="https://github.com/gogojjh/cobra" rel="external nofollow noopener" target="_blank">Code</a> 
and 
<a href="https://drive.google.com/drive/folders/160aA4naMKBFRpjt8f0LUYCYSrWYrER5G?usp=sharing" rel="external nofollow noopener" target="_blank">Dataset</a>
AvA.
</h4>

<h3 id="demo">Demo</h3>
<p align="center">
  <img src="/assets/img/2024_tase_mapping/mapping_hkustgz.gif" width="70%">
  </p>
<p align="center">The semantic mapping process. </p>

<p align="center">
  <img src="/assets/img/2024_tase_mapping/mapping_result_path_planning_hkustgz.png" width="70%">
  </p>
<p align="center">Traversability estimation and motion planning on the resulting occupancy map.</p>


<h3 id="background">Background</h3>
<p>Mapping is the process of establishing an internal representation of environments. Metric-semantic maps, which include human-readable information (<em>i.e.,</em> where map elements are labeled), offer a more profound understanding of environments. This contrasts with pure metric maps, which primarily store the geometric structure of a scene. These are typically defined by the positions of landmarks (known as a point cloud map), distances to obstacles (referred to as a distance field), or binary values that represent free and occupied spaces (occupancy map).
Recent works (shwon below) in scene abstraction (<em>e.g.,</em> <a href="https://github.com/MIT-SPARK/Hydra" rel="external nofollow noopener" target="_blank">Hydra</a>), long-term map update (<em>e.g.,</em> <a href="https://github.com/ethz-asl/panoptic_mapping" rel="external nofollow noopener" target="_blank">Panoptic Mapping</a>), exploration (<em>e.g.,</em> <a href="https://devendrachaplot.github.io/projects/semantic-exploration" rel="external nofollow noopener" target="_blank">Semantic Exploration</a>, and grasping (<em>e.g.,</em> <a href="https://theophilegervet.github.io/projects/goat" rel="external nofollow noopener" target="_blank">GOAT</a>) have demonstrated the potential of semantic maps, boosting the development of embodied intelligence.</p>

<p align="center">
  <img src="/assets/img/2024_tase_mapping/example_hydra.png" width="40%">
  <img src="/assets/img/2024_tase_mapping/example_goat.png" width="50%">
  <img src="/assets/img/2024_tase_mapping/example_panoptic_mapping.png" width="50%">
  <img src="/assets/img/2024_tase_mapping/example_semantic_exploration.png" width="70%">
</p>

<p>Taking the KITTI scenario as an example. Many objects such as trees and buildings appear. The street is also composed of the sidewalk that is specifically designed for pedestrains. By incorporating semantics, the map enables the vehicle to navigate along the road, finding a path that is free of collisions and avoids intersecting with sidewalks and grasslands. In contrast, geometry-based traversability extraction methods often face challenges in distinguishing between roads, sidewalks, and grass due to their similar structures. This paper focuses on the point-goal navigation task of ground robots in complicated unstructured environments (<em>e.g.,</em> campus, off-road scenarios), where abundant semantic elements should be recognized to guarantee safe and highly-interactive navigation.</p>
<p align="center">
  <img src="/assets/img/2024_tase_mapping/example_semantickitti.png" width="60%">
</p>

<h3 id="methodology">Methodology</h3>

<p>The overview of the method is shown as below. Please refer to the preprint for more technical details.</p>

<p align="center">
  <img src="/assets/img/2024_tase_mapping/pipeline.png" width="60%">
</p>

<h4 id="metric-semantic-mapping">Metric-Semantic Mapping</h4>

<p>Building a semantic map for large-scale outdoor environments costs much time. Therefore, in this work, we propose an real-time <strong>metric-semantic mapping system</strong> which leverages LiDAR-visual-inertial sensing to <strong>estimate the real-time state</strong> of the robot and <strong>construct a lighweight and global metric-semantic mesh map</strong> of the environment. We build upon the work of <a href="https://github.com/nvidia-isaac/nvblox" rel="external nofollow noopener" target="_blank">NvBlox</a> and thus utilize a <em>signed distance field (SDF)</em>-based representation. This representation offers the advantage of constructing surfaces with sub-voxel resolution, enhancing the accuracy of the map. While the focus of this paper is on mapping outdoor environments, the proposed solution is both extensible and easily adaptable for above applications.
We publicly release our code and datasets in <a href="https://github.com/HKUSTGZ-IADC/cobra" rel="external nofollow noopener" target="_blank">Cobra</a>.
The mapping system consists of four primary components:</p>

<ol>
  <li>
<strong>State Estimator</strong> modifies the <a href="https://github.com/hku-mars/r3live" rel="external nofollow noopener" target="_blank">R3LIVE</a> system (an Extended Kalman Filter-based LiDAR-visual-inertial odometry) to estimate real-time sensors’ poses with a local and sparse color point cloud.</li>
  <li>
<strong>Semantic Segmentation</strong> is a pre-trained convolutional neural network (CNN) that assigns a class label to every single pixel of each input image. A novel dataset that categorizes objects into diverse classes for the network training is also developed.</li>
  <li>
<strong>Metric-Semantic Mapping</strong> takes sensors’ measurements and poses as input, and constructs a 3D global mesh of environments using the TSDF-based volumetric representation with 2D semantic segmentation. The mapping is implemented in parallel with the GPU and thus achieves the real-time performance (&lt;<em>7ms</em> per LiDAR frame). We also propose a non-projective distance under the TSDF formulation, leading to a more accurate and complete surface reconstruction.</li>
  <li>
<strong>Traversability Analysis</strong> identifies drivable areas by analyzing the geometric and semantic attributes of the resulting mesh map, thus narrowing the search space for subsequent motion planning. For example, we can classify ‘‘road’’ regions as drivable for vehicles, while ‘‘sidewalk’’ or ‘‘grass’’ regions are not.</li>
</ol>

<h4 id="navigation-system-integration">Navigation System Integration</h4>

<p>We also integrate the resuting map into a navigation system for a real-world autonomous vehicle. The semantic data encoded in the map translates human instructions, thereby enabling robots to navigate safely within unstructured environments. Implementation details are summarized:</p>

<ul>
  <li>
<strong>Prior Map-based Localization</strong>: We use <a href="https://arxiv.org/abs/2401.17826" rel="external nofollow noopener" target="_blank">PALoc</a> to obtain the real-time global pose of the vehicle by registering the map of the current scan.</li>
  <li>
<strong>Map Process and Motion Planning</strong>: We transform the vertices of the traversable map into a 2D occupancy grid, where cells with zero occupancy probability are considered drivable. And then we employ a hybrid A star algorithm for motion planning.</li>
  <li>
<strong>Path Following</strong>: Waypoints are taken as input of the further path following module with the lateral trajectory tracking controller (for steering adjustments) and a longitudinal speed controller (for speed changes).</li>
</ul>

<h3 id="experimental-results">Experimental Results</h3>
<!-- 
#### Dataset

* **SemanticKITTI**: street and urban road with GT LiDAR semantics.
* **SemanticUSL**: campus-type and off-road scenarios with GT LiDAR semantics.
* **FusionPortable**: campus-type scenarios with estimated camera semantics. -->

<h4 id="real-world-experimental-platform">Real-World Experimental Platform</h4>

<p align="center">
  <img src="/assets/img/2024_tase_mapping/experiment_platform.png" width="50%">
  </p>
<p align="center">Real-world experimental platform.</p>


<h4 id="some-mapping-results">Some Mapping Results</h4>
<p align="center">
  &lt;img src="/assets/img/2024_tase_mapping/mapping_semantickitti.gif" width=70%" /&gt;
  </p>
<p align="center">Test on SemanticKITTI</p>


<p align="center">
  <img src="/assets/img/2024_tase_mapping/mapping_fusionportable.gif" width="70%">
  </p>
<p align="center">Test on FusionPortable</p> 


<h4 id="real-world-navigation-results">Real-World Navigation Results</h4>

<p align="center">
  <img src="/assets/img/2024_tase_mapping/navigation_experiment_sequence00.gif" width="70%">
	</p>
<p align="center">Point-goal navigation test</p>

<p align="center">
  <img src="/assets/img/2024_tase_mapping/navigation_experiment_other_sequence.gif" width="70%">
  </p>
<p align="center">Other point-goal navigation tests</p>


<h4 id="more-results">More Results</h4>
<p>To demonstrate the effects of concerning the impact of measurement noise, varying view angles, and limited observations on mapping, we have conducted a series of supplementary experiments using the MaiCity dataset sequence 01, as compared with baseline methods.</p>
<p align="center">
  <img src="/assets/img/2024_tase_mapping/2024_tase_comp_table.png" width="70%">
</p>

<h3 id="citation">Citation</h3>
<p>Please cite our paper if you find the code and dataset useful to your research:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{jiao2024real,
  title={Real-Time Metric-Semantic Mapping for Autonomous Navigation in Outdoor Environments},
  author={Jiao, Jianhao and Geng, Ruoyu and Li, Yuanhang and Xin, Ren and Yang, Bowen and Wu, Jin and Wang, Lujia and Liu, Ming and Fan, Rui and Kanoulas, Dimitrios},
  journal={IEEE Transactions on Automation Science and Engineering},
  year={2024},
  publisher={IEEE}
}
</code></pre></div></div>
<p>Dataset:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{jiao2022fusionportable,
  title={FusionPortable: A Multi-Sensor Campus-Scene Dataset for Evaluation of Localization and Mapping Accuracy on Diverse Platforms},
  author={Jiao, Jianhao and Wei, Hexiang and Hu, Tianshuai and Hu, Xiangcheng and Zhu, Yilong and He, Zhijian and Wu, Jin and Yu, Jingwen and Xie, Xupeng and Huang, Huaiyang and others},
  booktitle={2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={3851--3856},
  year={2022},
  organization={IEEE}
}
</code></pre></div></div>

          </article>

        </div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Jianhao  Jiao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.3/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
